{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4159be0e",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2022-04-08 Optimization\n",
    "\n",
    "## Last time\n",
    "\n",
    "* Project feedback\n",
    "* Computing derivatives in maintainable code\n",
    "* Forward and reverse\n",
    "* Algorithmic (automatic) differentiation\n",
    "* Projects\n",
    "\n",
    "## Today\n",
    "* Reflect on differentiation\n",
    "* Second order (Newton type) optimization\n",
    "* Project discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb781a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff_wp (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using Plots\n",
    "default(linewidth=4, legendfontsize=12)\n",
    "\n",
    "function vander(x, k=nothing)\n",
    "    if isnothing(k)\n",
    "        k = length(x)\n",
    "    end\n",
    "    m = length(x)\n",
    "    V = ones(m, k)\n",
    "    for j in 2:k\n",
    "        V[:, j] = V[:, j-1] .* x\n",
    "    end\n",
    "    V\n",
    "end\n",
    "\n",
    "function vander_chebyshev(x, n=nothing)\n",
    "    if isnothing(n)\n",
    "        n = length(x) # Square by default\n",
    "    end\n",
    "    m = length(x)\n",
    "    T = ones(m, n)\n",
    "    if n > 1\n",
    "        T[:, 2] = x\n",
    "    end\n",
    "    for k in 3:n\n",
    "        #T[:, k] = x .* T[:, k-1]\n",
    "        T[:, k] = 2 * x .* T[:,k-1] - T[:, k-2]\n",
    "    end\n",
    "    T\n",
    "end\n",
    "\n",
    "runge(x) = 1 / (1 + 10*x^2)\n",
    "runge_noisy(x, sigma) = runge.(x) + randn(size(x)) * sigma\n",
    "\n",
    "function grad_descent(loss, grad, c0; gamma=1e-3, tol=1e-5)\n",
    "    \"\"\"Minimize loss(c) via gradient descent with initial guess c0\n",
    "    using learning rate gamma.  Declares convergence when gradient\n",
    "    is less than tol or after 500 steps.\n",
    "    \"\"\"\n",
    "    c = copy(c0)\n",
    "    chist = [copy(c)]\n",
    "    lhist = [loss(c)]\n",
    "    for it in 1:500\n",
    "        g = grad(c)\n",
    "        c -= gamma * g\n",
    "        push!(chist, copy(c))\n",
    "        push!(lhist, loss(c))\n",
    "        if norm(g) < tol\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    (c, hcat(chist...), lhist)\n",
    "end\n",
    "\n",
    "function diff_wp(f, x; eps=1e-8)\n",
    "    \"\"\"Diff using Walker and Pernice (1998) choice of step\"\"\"\n",
    "    h = eps * (1 + abs(x))\n",
    "    (f(x+h) - f(x)) / h\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a814a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hand-coding derivatives\n",
    "\n",
    "$$ df = f'(x) dx $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c27ac54",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.5346823414986814, -34.032439961925064)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f(x)\n",
    "    y = x\n",
    "    for _ in 1:2\n",
    "        a = y^pi\n",
    "        b = cos(a)\n",
    "        c = log(y)\n",
    "        y = b * c\n",
    "    end\n",
    "    y\n",
    "end\n",
    "\n",
    "f(1.9), diff_wp(f, 1.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8527e788",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.5346823414986814, -34.03241959914048)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function df(x, dx)\n",
    "    y = x\n",
    "    dy = dx\n",
    "    for _ in 1:2\n",
    "        a = y ^ pi\n",
    "        da = pi * y^(pi - 1) * dy\n",
    "        b = cos(a)\n",
    "        db = -sin(a) * da\n",
    "        c = log(y)\n",
    "        dc = 1/y * dy\n",
    "        y = b * c\n",
    "        dy = db * c + b * dc\n",
    "    end\n",
    "    y, dy\n",
    "end\n",
    "\n",
    "df(1.9, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe488cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We can go the other way\n",
    "\n",
    "We can differentiate a composition $h(g(f(x)))$ as\n",
    "\n",
    "\\begin{align}\n",
    "  \\operatorname{d} h &= h' \\operatorname{d} g \\\\\n",
    "  \\operatorname{d} g &= g' \\operatorname{d} f \\\\\n",
    "  \\operatorname{d} f &= f' \\operatorname{d} x.\n",
    "\\end{align}\n",
    "\n",
    "What we've done above is called \"forward mode\", and amounts to placing the parentheses in the chain rule like\n",
    "\n",
    "$$ \\operatorname d h = \\frac{dh}{dg} \\left(\\frac{dg}{df} \\left(\\frac{df}{dx} \\operatorname d x \\right) \\right) .$$\n",
    "\n",
    "The expression means the same thing if we rearrange the parentheses,\n",
    "\n",
    "$$ \\operatorname d h = \\left( \\left( \\left( \\frac{dh}{dg} \\right) \\frac{dg}{df} \\right) \\frac{df}{dx} \\right) \\operatorname d x $$\n",
    "\n",
    "which we can compute with in reverse order via\n",
    "\n",
    "$$ \\underbrace{\\bar x}_{\\frac{dh}{dx}} = \\underbrace{\\bar g \\frac{dg}{df}}_{\\bar f} \\frac{df}{dx} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5c89e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A reverse mode example\n",
    "\n",
    "$$ \\underbrace{\\bar x}_{\\frac{dh}{dx}} = \\underbrace{\\bar g \\frac{dg}{df}}_{\\bar f} \\frac{df}{dx} .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a141faf",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.32484122107701546, -1.2559760384500684)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function g(x)\n",
    "    a = x^pi\n",
    "    b = cos(a)\n",
    "    c = log(x)\n",
    "    y = b * c\n",
    "    y\n",
    "end\n",
    "(g(1.4), diff_wp(g, 1.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79bd7d27",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2559761698835525"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gback(x, y_)\n",
    "    a = x^pi\n",
    "    b = cos(a)\n",
    "    c = log(x)\n",
    "    y = b * c\n",
    "    # backward pass\n",
    "    c_ = y_ * b \n",
    "    b_ = c * y_\n",
    "    a_ = -sin(a) * b_\n",
    "    x_ = 1/x * c_ + pi * x^(pi-1) * a_\n",
    "    x_\n",
    "end\n",
    "gback(1.4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971729c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d2f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0289826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-34.03241959914049,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient(f, 1.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a4ea4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does Zygote work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "443c9a8f",
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m;  @ In[13]:1 within `square`\u001b[39m\n",
      "\u001b[95mdefine\u001b[39m \u001b[36mdouble\u001b[39m \u001b[93m@julia_square_5308\u001b[39m\u001b[33m(\u001b[39m\u001b[36mdouble\u001b[39m \u001b[0m%0\u001b[33m)\u001b[39m \u001b[0m#0 \u001b[33m{\u001b[39m\n",
      "\u001b[91mtop:\u001b[39m\n",
      "\u001b[90m; ┌ @ intfuncs.jl:312 within `literal_pow`\u001b[39m\n",
      "\u001b[90m; │┌ @ float.jl:405 within `*`\u001b[39m\n",
      "    \u001b[0m%1 \u001b[0m= \u001b[96m\u001b[1mfmul\u001b[22m\u001b[39m \u001b[36mdouble\u001b[39m \u001b[0m%0\u001b[0m, \u001b[0m%0\n",
      "\u001b[90m; └└\u001b[39m\n",
      "  \u001b[96m\u001b[1mret\u001b[22m\u001b[39m \u001b[36mdouble\u001b[39m \u001b[0m%1\n",
      "\u001b[33m}\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "square(x) = x^2\n",
    "@code_llvm square(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bee4c0a7",
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m;  @ /home/jed/.julia/packages/Zygote/H6vD3/src/compiler/interface.jl:74 within `gradient`\u001b[39m\n",
      "\u001b[95mdefine\u001b[39m \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mdouble\u001b[39m\u001b[33m]\u001b[39m \u001b[93m@julia_gradient_5425\u001b[39m\u001b[33m(\u001b[39m\u001b[36mdouble\u001b[39m \u001b[0m%0\u001b[33m)\u001b[39m \u001b[0m#0 \u001b[33m{\u001b[39m\n",
      "\u001b[91mtop:\u001b[39m\n",
      "\u001b[90m;  @ /home/jed/.julia/packages/Zygote/H6vD3/src/compiler/interface.jl:76 within `gradient`\u001b[39m\n",
      "\u001b[90m; ┌ @ /home/jed/.julia/packages/Zygote/H6vD3/src/compiler/interface.jl:41 within `#56`\u001b[39m\n",
      "\u001b[90m; │┌ @ In[13]:1 within `Pullback`\u001b[39m\n",
      "\u001b[90m; ││┌ @ /home/jed/.julia/packages/ZygoteRules/AIbCs/src/adjoint.jl:67 within `#1822#back`\u001b[39m\n",
      "\u001b[90m; │││┌ @ /home/jed/.julia/packages/Zygote/H6vD3/src/lib/number.jl:6 within `#254`\u001b[39m\n",
      "\u001b[90m; ││││┌ @ promotion.jl:380 within `*` @ float.jl:405\u001b[39m\n",
      "       \u001b[0m%1 \u001b[0m= \u001b[96m\u001b[1mfmul\u001b[22m\u001b[39m \u001b[36mdouble\u001b[39m \u001b[0m%0\u001b[0m, \u001b[33m2.000000e+00\u001b[39m\n",
      "\u001b[90m; └└└└└\u001b[39m\n",
      "\u001b[90m;  @ /home/jed/.julia/packages/Zygote/H6vD3/src/compiler/interface.jl:77 within `gradient`\u001b[39m\n",
      "  \u001b[0m%.fca.0.insert \u001b[0m= \u001b[96m\u001b[1minsertvalue\u001b[22m\u001b[39m \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mdouble\u001b[39m\u001b[33m]\u001b[39m \u001b[95mzeroinitializer\u001b[39m\u001b[0m, \u001b[36mdouble\u001b[39m \u001b[0m%1\u001b[0m, \u001b[33m0\u001b[39m\n",
      "  \u001b[96m\u001b[1mret\u001b[22m\u001b[39m \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mdouble\u001b[39m\u001b[33m]\u001b[39m \u001b[0m%.fca.0.insert\n",
      "\u001b[33m}\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@code_llvm Zygote.gradient(square, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f07764",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kinds of algorithmic differentation\n",
    "\n",
    "* Source transformation: Fortran code in, Fortran code out\n",
    "  * Duplicates compiler features, usually incomplete language coverage\n",
    "  * Produces efficient code\n",
    "* Operator overloading: C++ types\n",
    "  * Hard to vectorize\n",
    "  * Loops are effectively unrolled/inefficient\n",
    "* Just-in-time compilation: tightly coupled with compiler\n",
    "  * JIT lag\n",
    "  * Needs dynamic language features (JAX) or tight integration with compiler (Zygote, Enzyme)\n",
    "  * Some [sharp bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975aeb0b",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Forward or reverse?\n",
    "\n",
    "It all depends on the shape.\n",
    "\n",
    "* One input, many outputs: use forward mode\n",
    "  * \"One input\" can be looking in one direction\n",
    "* Many inputs, one output: use reverse mode\n",
    "  * Will need to traverse execution backwards (\"tape\")\n",
    "  * Hierarchical checkpointing\n",
    "* Small intermediate results?\n",
    "  * Break into pieces and use forward/reverse for the smaller pieces.\n",
    "* About square? Forward is usually a bit more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcaa9b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can you differentiate an algorithm?\n",
    "\n",
    "* Input $c$, output $x$ such that $f(x,c) = 0$\n",
    "* Input $A$, output $\\lambda$ such that $A x = \\lambda x$ for some nonzero vector $x$\n",
    "* Input `buffer`, output `sha256(buffer)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c744f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ill-conditioned optimization\n",
    "\n",
    "$$ L(c; x, y) = \\frac 1 2 \\lVert \\underbrace{f(x, c) - y}_{r(c)} \\rVert_{C^{-1}}^2 $$\n",
    "\n",
    "Gradient of $L$ requires the Jacobian $J$ of the model $f$.\n",
    "\n",
    "$$ g(c) = \\nabla_c L = r^T \\underbrace{\\nabla_c f}_{J} $$\n",
    "\n",
    "We can solve $g(c) = 0$ using a Newton method\n",
    "\n",
    "$$ g(c + \\delta c) = g(c) + \\underbrace{\\nabla_c g}_H\\delta c + O((\\delta c)^2) $$\n",
    "\n",
    "The Hessian requires the second derivative of $f$, which can cause problems.\n",
    "\n",
    "$$ H = J^T J + r^T (\\nabla_c J)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae77276",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Newton-like methods for optimization\n",
    "\n",
    "Solve\n",
    "\n",
    "$$ H \\delta c = -g(c) $$\n",
    "\n",
    "Update $c \\gets c + \\gamma \\delta c$ using a line search or trust region.\n",
    "\n",
    "* Gauss-Newton: $H = J^T J$\n",
    "* Levenberg-Marquardt: $H = J^T J + \\alpha I$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1743274",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outlook\n",
    "\n",
    "* The optimization problem can be solved using a Newton method.  It can be onerous to implement the needed derivatives.\n",
    "* The [Gauss-Newton method](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm) is often more practical than Newton while being faster than gradient descent, though it lacks robustness.\n",
    "* The [Levenberg-Marquardt method](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) provides a sort of middle-ground between Gauss-Newton and gradient descent.\n",
    "* Many globalization techniques are used for models that possess many local minima.\n",
    "* One pervasive approach is stochastic gradient descent, where small batches (e.g., 1 or 10 or 20) are selected randomly from the corpus of observations (500 in our current example), and a step of gradient descent is applied to that reduced set of observations.  This helps to escape saddle points and weak local minima.\n",
    "* Among expressive models $f(x,c)$, some may converge much more easily than others.  Having a good optimization algorithm is essential for nonlinear regression with complicated models, especially those with many parameters $c$.\n",
    "* Classification is a very similar problem to regression, but the observations $y$ are discrete, thus\n",
    "\n",
    " * models $f(x,c)$ must have discrete output\n",
    " * the least squares loss function is not appropriate.\n",
    "* [Why momentum really works](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0205b32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Team project workshopping\n",
    "\n",
    "1. Identify one or more software packages and execution environments that will be used.\n",
    "2. Identify one or more stakeholders in this problem space.\n",
    "3. Identify one or more decisions that depend on the use or behavior of the methods or software.\n",
    "4. What will you do?\n",
    "  * Identify a test you can conduct to inform the decision. Think about the stakeholders and context.\n",
    "  * Create something new using the software and reflect on how it works.\n",
    "  * Make a contribution to improve the software or community (diagnostics, decumentation, efficiency). (Please share your ideas with me before choosing this.)\n",
    "5. Create a thread in `#projects` summarizing the above and naming your group members."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
